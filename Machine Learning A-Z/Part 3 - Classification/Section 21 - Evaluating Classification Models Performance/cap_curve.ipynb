{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part3 - Classification - CAP Curve & cap analysis\n",
    "\n",
    "- 학습 날짜 : 2019 - 03 - 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note \n",
    "\n",
    "![alt text](cap_curve_1.png)\n",
    "![alt text](cap_curve_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cap analysis\n",
    "\n",
    "#### 면적을 구하는 방법\n",
    "![alt text](cap_curve_3.png)\n",
    "\n",
    "\n",
    "#### 50% 기준으로 대응되는 값(%)에 대한 판단\n",
    "![alt text](cap_curve_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary part 3\n",
    "\n",
    "![alt text](part_3_model_comparision.png)\n",
    "\n",
    "In this Part 3 you learned about 7 classification models. Like for Part 2 - Regression, that's quite a lot so you might be asking yourself the same questions as before:\n",
    "\n",
    "What are the pros and cons of each model ?\n",
    "How do I know which model to choose for my problem ?\n",
    "How can I improve each of these models ?\n",
    "Again, let's answer each of these questions one by one:\n",
    "\n",
    "1. What are the pros and cons of each model ?\n",
    "\n",
    "Please find here a cheat-sheet that gives you all the pros and the cons of each classification model.\n",
    "\n",
    "2. How do I know which model to choose for my problem ?\n",
    "\n",
    "Same as for regression models, you first need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then:\n",
    "\n",
    "If your problem is linear, you should go for Logistic Regression or SVM.\n",
    "\n",
    "If your problem is non linear, you should go for K-NN, Naive Bayes, Decision Tree or Random Forest.\n",
    "\n",
    "Then which one should you choose in each case ? You will learn that in Part 10 - Model Selection with k-Fold Cross Validation.\n",
    "\n",
    "Then from a business point of view, you would rather use:\n",
    "\n",
    "- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.\n",
    "\n",
    "- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.\n",
    "\n",
    "- Decision Tree when you want to have clear interpretation of your model results,\n",
    "\n",
    "- Random Forest when you are just looking for high performance with less need for interpretation. \n",
    "\n",
    "3. How can I improve each of these models ?\n",
    "\n",
    "Same answer as in Part 2: \n",
    "\n",
    "In Part 10 - Model Selection, you will find the second section dedicated to Parameter Tuning, that will allow you to improve the performance of your models, by tuning them. You probably already noticed that each model is composed of two types of parameters:\n",
    "\n",
    "the parameters that are learnt, for example the coefficients in Linear Regression,\n",
    "the hyperparameters.\n",
    "The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value of these hyperparameters, and we haven't searched for their optimal value so that your model reaches even higher performance. Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection.\n",
    "\n",
    "Now congratulations for having completed Part 3, and let's move on the next part of the journey:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting and underfitting\n",
    "[overfitting and underfitting](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)\n",
    "\n",
    "https://www.youtube.com/watch?v=5wQ64XqQQhQ\n",
    "\n",
    "![alt text](bias_variance.png)\n",
    "\n",
    "### how to overcome underfitting?\n",
    "- Find more features\n",
    "- Try high variance machine learning models\n",
    "  - decision Tree, k-nn, svm\n",
    "  \n",
    "### how to find if overfitting?\n",
    "- when test error is much higher than training error\n",
    "\n",
    "- validation set을 갖추면 된다.!!\n",
    "\n",
    "#### K-Folds Cross Validation\n",
    "1. during training, test against validation data\n",
    "2. if validation accuracy is lower, do regularization!\n",
    "3. repeat regularization until no overfitting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- overfitting 에서 조금 더 smooth하게 바꿔주는 것을 Regularization이라고 볼 수 있다.\n",
    "\n",
    "![alt text](regularization.png)\n",
    "\n",
    "#### cost Function of regression\n",
    "- 예측 값과 실제 값의 cost function이 작으면 됩니다.\n",
    "- 계수 값을 작게 만들어서 기울기를 좀 더 완만하게 만들어 준다.\n",
    "- 세타 값은 함수의 계수값들입니다.\n",
    "\n",
    "![alt text](regularization_1.png)\n",
    "![alt text](regularization_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](regularization_3.png)\n",
    "![alt text](regularization_4.png)\n",
    "![alt text](regularization_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping - deep learning\n",
    "- training과 validation이 적정한 수치일 때 stop하는 기법\n",
    "\n",
    "![alt text](Early_Stopping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop out - deep learning\n",
    "- ensemble의 bagging과 비슷한 기능\n",
    "- neuron을 선택적으로 사용\n",
    "- sub group들이 학습이 되었고, 모든 것을 학습한 건 아니니까 variance가 낮아짐\n",
    "- overtraining이 덜 되니까 overfitting에서 멀어짐\n",
    "\n",
    "![alt text](drop_out.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
