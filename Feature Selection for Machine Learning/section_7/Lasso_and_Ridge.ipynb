{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regularisation\n",
    "-------\n",
    "\n",
    "Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wontaek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/wontaek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/wontaek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/wontaek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/Users/wontaek/Documents/Lecture_dataset/BNP_Paribas_Cardif_claims/train.csv'\n",
    "data = pd.read_csv(file_path, nrows=50000)\n",
    "data.shape\n",
    "\n",
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape\n",
    "\n",
    "\n",
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear models benefit from feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Meta-transformer for selecting features based on importance weights.\n",
       "\n",
       ".. versionadded:: 0.17\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : object\n",
       "    The base estimator from which the transformer is built.\n",
       "    This can be both a fitted (if ``prefit`` is set to True)\n",
       "    or a non-fitted estimator. The estimator must have either a\n",
       "    ``feature_importances_`` or ``coef_`` attribute after fitting.\n",
       "\n",
       "threshold : string, float, optional default None\n",
       "    The threshold value to use for feature selection. Features whose\n",
       "    importance is greater or equal are kept while the others are\n",
       "    discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n",
       "    the median (resp. the mean) of the feature importances. A scaling\n",
       "    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n",
       "    estimator has a parameter penalty set to l1, either explicitly\n",
       "    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n",
       "    Otherwise, \"mean\" is used by default.\n",
       "\n",
       "prefit : bool, default False\n",
       "    Whether a prefit model is expected to be passed into the constructor\n",
       "    directly or not. If True, ``transform`` must be called directly\n",
       "    and SelectFromModel cannot be used with ``cross_val_score``,\n",
       "    ``GridSearchCV`` and similar utilities that clone the estimator.\n",
       "    Otherwise train the model using ``fit`` and then ``transform`` to do\n",
       "    feature selection.\n",
       "\n",
       "norm_order : non-zero int, inf, -inf, default 1\n",
       "    Order of the norm used to filter the vectors of coefficients below\n",
       "    ``threshold`` in the case where the ``coef_`` attribute of the\n",
       "    estimator is of dimension 2.\n",
       "\n",
       "max_features : int or None, optional\n",
       "    The maximum number of features selected scoring above ``threshold``.\n",
       "    To disable ``threshold`` and only select based on ``max_features``,\n",
       "    set ``threshold=-np.inf``.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimator_ : an estimator\n",
       "    The base estimator from which the transformer is built.\n",
       "    This is stored only when a non-fitted estimator is passed to the\n",
       "    ``SelectFromModel``, i.e when prefit is False.\n",
       "\n",
       "threshold_ : float\n",
       "    The threshold value used for feature selection.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/from_model.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SelectFromModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                             fit_intercept=True,\n",
       "                                             intercept_scaling=1, l1_ratio=None,\n",
       "                                             max_iter=100, multi_class='warn',\n",
       "                                             n_jobs=None, penalty='l1',\n",
       "                                             random_state=None, solver='warn',\n",
       "                                             tol=0.0001, verbose=0,\n",
       "                                             warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I will do the model fitting and feature selection\n",
    "# altogether in one line of code\n",
    "\n",
    "# first I specify the Logistic Regression model, and I\n",
    "# make sure I select the Lasso (l1) penalty.\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn, which\n",
    "# will select in theory the features which coefficients are non-zero\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True, False, False,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True, False,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True,  True, False, False,\n",
       "       False, False,  True,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True, False,  True,\n",
       "        True,  True,  True, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were kept\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 112\n",
      "selected features: 79\n",
      "features with coefficients shrank to zero: 33\n"
     ]
    }
   ],
   "source": [
    "# Now I make a list with the selected features\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05409264,  0.11261324, -0.01775505,  0.08203875,  0.18097962,\n",
       "         0.04698098,  0.22232209,  0.0172384 ,  1.07241271,  0.        ,\n",
       "        -1.30044837, -0.05759033,  0.11693635, -0.14017872, -0.16478723,\n",
       "        -0.17934891,  0.03814743,  0.01265627,  0.        , -0.02968195,\n",
       "         0.06337596,  0.17883558, -0.03230851, -0.02365772, -0.00386115,\n",
       "         0.        ,  0.02579414,  0.        ,  0.77872597,  0.10769527,\n",
       "        -0.13186004,  0.        ,  0.27091026, -0.01308322,  0.16711316,\n",
       "         0.        ,  0.        , -0.03796082,  0.0654757 , -0.18652269,\n",
       "        -0.34461832,  0.        ,  0.        ,  0.31432984,  0.0366303 ,\n",
       "         0.        ,  0.02008912,  0.        ,  0.07331628, -0.08013521,\n",
       "         0.29939859, -0.08455828, -0.30384394,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.13630567,\n",
       "         0.12961183,  0.02536176,  0.21097968, -0.0380792 , -0.33272399,\n",
       "        -0.16327186, -0.0035045 ,  0.        , -0.05194252, -0.12991123,\n",
       "         0.02948778,  0.02681553,  0.        ,  0.        ,  0.        ,\n",
       "        -0.02017568,  0.1436873 ,  0.02220232,  0.28304147,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.1398361 , -0.00326802,\n",
       "        -0.12021678, -0.0597149 ,  0.02842647,  0.        ,  0.        ,\n",
       "        -0.02533285,  0.        , -0.18553954,  0.14640275,  0.27124832,\n",
       "        -0.45025246,  0.        ,  0.04021848, -0.1445828 ,  0.11221204,\n",
       "        -0.00358068,  0.01908131,  0.        ,  0.        , -0.16557599,\n",
       "        -0.00872985,  0.        , -0.03402608,  0.01971667,  0.30057508,\n",
       "        -0.08313035,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치를 0으로 만들어 버렸다.\n",
    "\n",
    "sel_.estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a contiguous flattened array.\n",
       "\n",
       "A 1-D array, containing the elements of the input, is returned.  A copy is\n",
       "made only if needed.\n",
       "\n",
       "As of NumPy 1.10, the returned array will have the same type as the input\n",
       "array. (for example, a masked array will be returned for a masked array\n",
       "input)\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like\n",
       "    Input array.  The elements in `a` are read in the order specified by\n",
       "    `order`, and packed as a 1-D array.\n",
       "order : {'C','F', 'A', 'K'}, optional\n",
       "\n",
       "    The elements of `a` are read using this index order. 'C' means\n",
       "    to index the elements in row-major, C-style order,\n",
       "    with the last axis index changing fastest, back to the first\n",
       "    axis index changing slowest.  'F' means to index the elements\n",
       "    in column-major, Fortran-style order, with the\n",
       "    first index changing fastest, and the last index changing\n",
       "    slowest. Note that the 'C' and 'F' options take no account of\n",
       "    the memory layout of the underlying array, and only refer to\n",
       "    the order of axis indexing.  'A' means to read the elements in\n",
       "    Fortran-like index order if `a` is Fortran *contiguous* in\n",
       "    memory, C-like order otherwise.  'K' means to read the\n",
       "    elements in the order they occur in memory, except for\n",
       "    reversing the data when strides are negative.  By default, 'C'\n",
       "    index order is used.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y : array_like\n",
       "    y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n",
       "    Note that matrices are special cased for backward compatibility, if `a`\n",
       "    is a matrix, then y is a 1-D ndarray.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ndarray.flat : 1-D iterator over an array.\n",
       "ndarray.flatten : 1-D array copy of the elements of an array\n",
       "                  in row-major order.\n",
       "ndarray.reshape : Change the shape of an array without changing its data.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In row-major, C-style order, in two dimensions, the row index\n",
       "varies the slowest, and the column index the quickest.  This can\n",
       "be generalized to multiple dimensions, where row-major order\n",
       "implies that the index along the first axis varies slowest, and\n",
       "the index along the last quickest.  The opposite holds for\n",
       "column-major, Fortran-style index ordering.\n",
       "\n",
       "When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n",
       "may be preferable.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "It is equivalent to ``reshape(-1, order=order)``.\n",
       "\n",
       ">>> x = np.array([[1, 2, 3], [4, 5, 6]])\n",
       ">>> print(np.ravel(x))\n",
       "[1 2 3 4 5 6]\n",
       "\n",
       ">>> print(x.reshape(-1))\n",
       "[1 2 3 4 5 6]\n",
       "\n",
       ">>> print(np.ravel(x, order='F'))\n",
       "[1 4 2 5 3 6]\n",
       "\n",
       "When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n",
       "\n",
       ">>> print(np.ravel(x.T))\n",
       "[1 4 2 5 3 6]\n",
       ">>> print(np.ravel(x.T, order='A'))\n",
       "[1 2 3 4 5 6]\n",
       "\n",
       "When ``order`` is 'K', it will preserve orderings that are neither 'C'\n",
       "nor 'F', but won't reverse axes:\n",
       "\n",
       ">>> a = np.arange(3)[::-1]; a\n",
       "array([2, 1, 0])\n",
       ">>> a.ravel(order='C')\n",
       "array([2, 1, 0])\n",
       ">>> a.ravel(order='K')\n",
       "array([2, 1, 0])\n",
       "\n",
       ">>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n",
       "array([[[ 0,  2,  4],\n",
       "        [ 1,  3,  5]],\n",
       "       [[ 6,  8, 10],\n",
       "        [ 7,  9, 11]]])\n",
       ">>> a.ravel(order='C')\n",
       "array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n",
       ">>> a.ravel(order='K')\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.ravel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sel_.estimator_.coef_ == 0).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v11', 'v20', 'v29', 'v33', 'v37', 'v41', 'v42', 'v48', 'v49', 'v53',\n",
       "       'v55', 'v62', 'v63', 'v64', 'v65', 'v67', 'v68', 'v81', 'v86', 'v87',\n",
       "       'v88', 'v94', 'v95', 'v96', 'v97', 'v103', 'v104', 'v106', 'v115',\n",
       "       'v121', 'v122', 'v126', 'v131'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can identify the removed features like this:\n",
    "removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.fillna(0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 79), (15000, 79))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then remove the features from the training and testing set\n",
    "# like this\n",
    "X_train_selected = sel_.transform(X_train.fillna(0))\n",
    "X_test_selected = sel_.transform(X_test.fillna(0))\n",
    "\n",
    "X_train_selected.shape, X_test_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularisation does not shrink coefficients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_logit = LogisticRegression(C=1, penalty='l2')\n",
    "l2_logit.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08644661,  0.11848921, -0.02066516,  0.09546023,  0.19054746,\n",
       "         0.20834857,  0.24117876,  0.08616941,  1.02850353,  0.03025465,\n",
       "        -1.266752  ,  0.01211486,  0.11856195, -0.17169921, -0.2301516 ,\n",
       "        -0.27243439,  0.04458787,  0.03295399, -0.23608815, -0.03093415,\n",
       "         0.06569448,  0.21791182, -0.03558688, -0.02002269, -0.00889964,\n",
       "        -0.05099209,  0.11136112,  0.22278852,  0.79323327,  0.20570042,\n",
       "        -0.16315617,  0.00724886,  0.18194687,  0.0075772 ,  0.17694584,\n",
       "         0.08960193, -0.10463332, -0.32323667,  0.0877821 , -0.22034634,\n",
       "        -0.39611284,  0.02752573, -0.28436817,  0.33044464,  0.04428683,\n",
       "         0.32332883,  0.02038317,  0.03885324,  0.15682228, -0.17900453,\n",
       "         0.32748066, -0.13913087, -0.40529031, -0.11110495,  0.01518036,\n",
       "         0.02659939, -0.15656106,  0.09889271,  0.12806048,  0.27403597,\n",
       "         0.171565  ,  0.17197311,  0.280187  , -0.00174109, -0.63020932,\n",
       "        -0.32268864, -0.04581572, -0.00180941, -0.06937573, -0.33675283,\n",
       "        -0.10048717,  0.03357096, -0.03661156, -0.0082675 , -0.03846991,\n",
       "        -0.02502584,  0.16899505, -0.01074196,  0.31420065,  0.0825413 ,\n",
       "         0.023245  ,  0.26910635, -0.06307594,  0.16471206, -0.00356887,\n",
       "        -0.24807811, -0.06969209,  0.02623657, -0.14531864,  0.03753423,\n",
       "        -0.04335276,  0.06527473, -0.21949002,  0.14590053,  0.34282832,\n",
       "        -0.4547757 ,  0.12370592,  0.34424633, -0.17055349,  0.24290615,\n",
       "        -0.00578269,  0.03215533, -0.05791768, -0.0563073 , -0.17601361,\n",
       "        -0.00601619, -0.10908427, -0.0413485 ,  0.0546084 ,  0.1885887 ,\n",
       "        -0.09485493, -0.02384039]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0으로 coef값을 낮추지는 않는다.\n",
    "\n",
    "l2_logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(l2_logit.coef_==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('/Users/wontaek/Documents/Lecture_dataset/House_Sale_Price/train.csv')\n",
    "data.shape\n",
    "\n",
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape\n",
    "\n",
    "\n",
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features in the house dataset are in very\n",
    "# different scales, so it helps the regression to scale\n",
    "# them\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Lasso(alpha=100, copy_X=True, fit_intercept=True,\n",
       "                                max_iter=1000, normalize=False, positive=False,\n",
       "                                precompute=False, random_state=None,\n",
       "                                selection='cyclic', tol=0.0001,\n",
       "                                warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, again I will train a Lasso Linear regression and select\n",
    "# the non zero features in one line.\n",
    "# bear in mind that the linear regression object from sklearn does\n",
    "# not allow for regularisation. So If you want to make a regularised\n",
    "# linear regression you need to import specifically \"Lasso\"\n",
    "# that is the l1 version of the linear regression\n",
    "# alpha is the penalisation here, so I set it high in order\n",
    "# to force the algorithm to shrink some coefficients\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 37\n",
      "selected features: 33\n",
      "features with coefficients shrank to zero: 4\n"
     ]
    }
   ],
   "source": [
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                                max_iter=None, normalize=False,\n",
       "                                random_state=None, solver='auto', tol=0.001),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(Ridge(alpha=1.0))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
       "       'MasVnrArea', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
       "       'GrLivArea', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageYrBlt',\n",
       "       'GarageCars', 'GarageArea'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
