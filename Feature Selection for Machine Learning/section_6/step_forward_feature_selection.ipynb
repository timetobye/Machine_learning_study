{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step forward feature selection\n",
    "----\n",
    "\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n",
    "\n",
    "This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: **mlxtend.**\n",
    "\n",
    "In the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features.\n",
    "\n",
    "Here I will use the Step Forward feature selection algorithm from mlxtend in a classification (Paribas) and regression (House Price) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step forward feature selection은 알고는 있지만 다시 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 133)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/Users/wontaek/Documents/Lecture_dataset/BNP_Paribas_Cardif_claims/train.csv'\n",
    "data = pd.read_csv(file_path, nrows=50000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 114)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  55\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "# in order to reduce the feature space a bit\n",
    "# so that the algorithm takes shorter\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 57), (15000, 57))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mk_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfloating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2*n_jobs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclone_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Sequential Feature Selection for Classification and Regression.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : scikit-learn classifier or regressor\n",
       "k_features : int or tuple or str (default: 1)\n",
       "    Number of features to select,\n",
       "    where k_features < the full feature set.\n",
       "    New in 0.4.2: A tuple containing a min and max value can be provided,\n",
       "        and the SFS will consider return any feature combination between\n",
       "        min and max that scored highest in cross-validtion. For example,\n",
       "        the tuple (1, 4) will return any combination from\n",
       "        1 up to 4 features instead of a fixed number of features k.\n",
       "    New in 0.8.0: A string argument \"best\" or \"parsimonious\".\n",
       "        If \"best\" is provided, the feature selector will return the\n",
       "        feature subset with the best cross-validation performance.\n",
       "        If \"parsimonious\" is provided as an argument, the smallest\n",
       "        feature subset that is within one standard error of the\n",
       "        cross-validation performance will be selected.\n",
       "forward : bool (default: True)\n",
       "    Forward selection if True,\n",
       "    backward selection otherwise\n",
       "floating : bool (default: False)\n",
       "    Adds a conditional exclusion/inclusion if True.\n",
       "verbose : int (default: 0), level of verbosity to use in logging.\n",
       "    If 0, no output,\n",
       "    if 1 number of features in current set, if 2 detailed logging i\n",
       "    ncluding timestamp and cv scores at step.\n",
       "scoring : str, callable, or None (default: None)\n",
       "    If None (default), uses 'accuracy' for sklearn classifiers\n",
       "    and 'r2' for sklearn regressors.\n",
       "    If str, uses a sklearn scoring metric string identifier, for example\n",
       "    {accuracy, f1, precision, recall, roc_auc} for classifiers,\n",
       "    {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n",
       "    'median_absolute_error', 'r2'} for regressors.\n",
       "    If a callable object or function is provided, it has to be conform with\n",
       "    sklearn's signature ``scorer(estimator, X, y)``; see\n",
       "    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
       "    for more information.\n",
       "cv : int (default: 5)\n",
       "    Integer or iterable yielding train, test splits. If cv is an integer\n",
       "    and `estimator` is a classifier (or y consists of integer class\n",
       "    labels) stratified k-fold. Otherwise regular k-fold cross-validation\n",
       "    is performed. No cross-validation if cv is None, False, or 0.\n",
       "n_jobs : int (default: 1)\n",
       "    The number of CPUs to use for evaluating different feature subsets\n",
       "    in parallel. -1 means 'all CPUs'.\n",
       "pre_dispatch : int, or string (default: '2*n_jobs')\n",
       "    Controls the number of jobs that get dispatched\n",
       "    during parallel execution if `n_jobs > 1` or `n_jobs=-1`.\n",
       "    Reducing this number can be useful to avoid an explosion of\n",
       "    memory consumption when more jobs get dispatched than CPUs can process.\n",
       "    This parameter can be:\n",
       "    None, in which case all the jobs are immediately created and spawned.\n",
       "        Use this for lightweight and fast-running jobs,\n",
       "        to avoid delays due to on-demand spawning of the jobs\n",
       "    An int, giving the exact number of total jobs that are spawned\n",
       "    A string, giving an expression as a function\n",
       "        of n_jobs, as in `2*n_jobs`\n",
       "clone_estimator : bool (default: True)\n",
       "    Clones estimator if True; works with the original estimator instance\n",
       "    if False. Set to False if the estimator doesn't\n",
       "    implement scikit-learn's set_params and get_params methods.\n",
       "    In addition, it is required to set cv=0, and n_jobs=1.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "k_feature_idx_ : array-like, shape = [n_predictions]\n",
       "    Feature Indices of the selected feature subsets.\n",
       "k_feature_names_ : array-like, shape = [n_predictions]\n",
       "    Feature names of the selected feature subsets. If pandas\n",
       "    DataFrames are used in the `fit` method, the feature\n",
       "    names correspond to the column names. Otherwise, the\n",
       "    feature names are string representation of the feature\n",
       "    array indices. New in v 0.13.0.\n",
       "k_score_ : float\n",
       "    Cross validation average score of the selected subset.\n",
       "subsets_ : dict\n",
       "    A dictionary of selected feature subsets during the\n",
       "    sequential selection, where the dictionary keys are\n",
       "    the lengths k of these feature subsets. The dictionary\n",
       "    values are dictionaries themselves with the following\n",
       "    keys: 'feature_idx' (tuple of indices of the feature subset)\n",
       "          'feature_names' (tuple of feature names of the feat. subset)\n",
       "          'cv_scores' (list individual cross-validation scores)\n",
       "          'avg_score' (average cross-validation score)\n",
       "    Note that if pandas\n",
       "    DataFrames are used in the `fit` method, the 'feature_names'\n",
       "    correspond to the column names. Otherwise, the\n",
       "    feature names are string representation of the feature\n",
       "    array indices. The 'feature_names' is new in v 0.13.0.\n",
       "\n",
       "Examples\n",
       "-----------\n",
       "For usage examples, please see\n",
       "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.6/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SFS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:   40.4s finished\n",
      "\n",
      "[2020-04-30 23:37:01] Features: 1/10 -- score: 0.6256432505795032[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:   33.3s finished\n",
      "\n",
      "[2020-04-30 23:37:34] Features: 2/10 -- score: 0.6426071460054256[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:   37.7s finished\n",
      "\n",
      "[2020-04-30 23:38:12] Features: 3/10 -- score: 0.6661052954407093[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:   44.9s finished\n",
      "\n",
      "[2020-04-30 23:38:57] Features: 4/10 -- score: 0.6477536192605601[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:   40.8s finished\n",
      "\n",
      "[2020-04-30 23:39:38] Features: 5/10 -- score: 0.6582510370307951[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:   38.7s finished\n",
      "\n",
      "[2020-04-30 23:40:16] Features: 6/10 -- score: 0.6643774692018946[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:   40.1s finished\n",
      "\n",
      "[2020-04-30 23:40:56] Features: 7/10 -- score: 0.6654281527717626[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   39.9s finished\n",
      "\n",
      "[2020-04-30 23:41:36] Features: 8/10 -- score: 0.6633167256050898[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:   43.8s finished\n",
      "\n",
      "[2020-04-30 23:42:20] Features: 9/10 -- score: 0.6608987155917411[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   43.6s finished\n",
      "\n",
      "[2020-04-30 23:43:04] Features: 10/10 -- score: 0.6630679098508573"
     ]
    }
   ],
   "source": [
    "# step forward feature selection\n",
    "# I indicate that I want to select 10 features from\n",
    "# the total, and that I want to select those features\n",
    "# based on the optimal roc_auc\n",
    "\n",
    "sfs1 = SFS(RandomForestClassifier(n_jobs=4), # CPU 프로세서가 4개면 4개 돌릴 수 있음\n",
    "           k_features=10, # feature 10개만 고를거다.\n",
    "           forward=True, # forward selection\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='roc_auc',\n",
    "           cv=3\n",
    "          )\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFeatureSelector(clone_estimator=True, cv=3,\n",
       "                          estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                           class_weight=None,\n",
       "                                                           criterion='gini',\n",
       "                                                           max_depth=None,\n",
       "                                                           max_features='auto',\n",
       "                                                           max_leaf_nodes=None,\n",
       "                                                           min_impurity_decrease=0.0,\n",
       "                                                           min_impurity_split=None,\n",
       "                                                           min_samples_leaf=1,\n",
       "                                                           min_samples_split=2,\n",
       "                                                           min_weight_fraction_leaf=0.0,\n",
       "                                                           n_estimators='warn',\n",
       "                                                           n_jobs=4,\n",
       "                                                           oob_score=False,\n",
       "                                                           random_state=None,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False),\n",
       "                          floating=False, forward=True, k_features=10, n_jobs=1,\n",
       "                          pre_dispatch='2*n_jobs', scoring='roc_auc',\n",
       "                          verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 8, 10, 18, 24, 27, 31, 40, 42, 55]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sfs1.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v5', 'v10', 'v13', 'v23', 'v34', 'v38', 'v50', 'v72', 'v82', 'v129'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    \n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    \n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.716502656474697\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7020288560849285\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# using selected features\n",
    "\n",
    "run_randomForests(X_train[selected_feat].fillna(0),\n",
    "                  X_test[selected_feat].fillna(0),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('/Users/wontaek/Documents/Lecture_dataset/House_Sale_Price//train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  3\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 34), (438, 34))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:    0.8s finished\n",
      "\n",
      "[2020-04-30 23:45:53] Features: 1/10 -- score: 0.666779083598836[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.8s finished\n",
      "\n",
      "[2020-04-30 23:45:54] Features: 2/10 -- score: 0.7214413607143048[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:    0.8s finished\n",
      "\n",
      "[2020-04-30 23:45:55] Features: 3/10 -- score: 0.7467711182022709[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    0.9s finished\n",
      "\n",
      "[2020-04-30 23:45:56] Features: 4/10 -- score: 0.769821133921381[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.9s finished\n",
      "\n",
      "[2020-04-30 23:45:56] Features: 5/10 -- score: 0.7707286174594586[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    0.8s finished\n",
      "\n",
      "[2020-04-30 23:45:57] Features: 6/10 -- score: 0.7680078730220387[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.8s finished\n",
      "\n",
      "[2020-04-30 23:45:58] Features: 7/10 -- score: 0.7673418394988097[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    0.9s finished\n",
      "\n",
      "[2020-04-30 23:45:59] Features: 8/10 -- score: 0.7947343338009182[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    1.2s finished\n",
      "\n",
      "[2020-04-30 23:46:00] Features: 9/10 -- score: 0.8242011571515638[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    1.2s finished\n",
      "\n",
      "[2020-04-30 23:46:01] Features: 10/10 -- score: 0.8405511265337314"
     ]
    }
   ],
   "source": [
    "# step forward feature selection\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 7, 14, 15, 16, 17, 19, 22, 24)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OverallQual', 'OverallCond', 'YearRemodAdd', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'KitchenAbvGr',\n",
       "       'GarageCars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature가 많으면 많을수록 기하급수적으로 시간이 오래 걸리는 단점이 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
