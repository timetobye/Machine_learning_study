{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustive feature selection\n",
    "-------\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all 15 feature combinations as follows:\n",
    "\n",
    "- all possible combinations of 1 feature\n",
    "- all possible combinations of 2 features\n",
    "- all possible combinations of 3 features\n",
    "- all the 4 features\n",
    "- and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n",
    "This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: mlxtend.\n",
    "\n",
    "In the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features.\n",
    "\n",
    "Here I will use the Exhaustive feature selection algorithm from mlxtend in a classification (Paribas) and regression (House Price) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 자료 - http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35000, 57), (15000, 57))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/Users/wontaek/Documents/Lecture_dataset/BNP_Paribas_Cardif_claims/train.csv'\n",
    "data = pd.read_csv(file_path, nrows=50000)\n",
    "data.shape\n",
    "\n",
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape\n",
    "\n",
    "\n",
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "\n",
    "# find and remove correlated features\n",
    "# in order to reduce the feature space a bit\n",
    "# so that the algorithm takes shorter\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )\n",
    "\n",
    "\n",
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조합을 만들어서 feature를 선택하는 방법이다.\n",
    "- 최소 조합의 개수와 최대 조합의 개수를 선택해서 하는 방식\n",
    "- feature가 많을 수록 경우의 수도 많으니 오래 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 31/31"
     ]
    }
   ],
   "source": [
    "# exhaustive feature selection\n",
    "# I indicate that I want to select 10 features from\n",
    "# the total, and that I want to select those features\n",
    "# based on the optimal roc_auc\n",
    "\n",
    "# in order to shorter search time for the demonstration\n",
    "# i will ask the algorithm to try all possible 1,2,3 and 4\n",
    "# feature combinations from a dataset of 4 features\n",
    "\n",
    "# if you have access to a multicore or distributed computer\n",
    "# system you can try more greedy searches\n",
    "\n",
    "efs1 = EFS(RandomForestClassifier(n_jobs=4, random_state=0), \n",
    "           min_features=1,\n",
    "           max_features=5, \n",
    "           scoring='roc_auc',\n",
    "           print_progress=True,\n",
    "           cv=2)\n",
    "\n",
    "efs1 = efs1.fit(np.array(X_train[X_train.columns[0:5]].fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    \n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    \n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature_idx': (0,),\n",
       "  'cv_scores': array([0.50410602, 0.5043581 ]),\n",
       "  'avg_score': 0.5042320625850624,\n",
       "  'feature_names': ('0',)},\n",
       " 1: {'feature_idx': (1,),\n",
       "  'cv_scores': array([0.50496613, 0.49612179]),\n",
       "  'avg_score': 0.5005439617005987,\n",
       "  'feature_names': ('1',)},\n",
       " 2: {'feature_idx': (2,),\n",
       "  'cv_scores': array([0.50457252, 0.50092121]),\n",
       "  'avg_score': 0.5027468671542781,\n",
       "  'feature_names': ('2',)},\n",
       " 3: {'feature_idx': (3,),\n",
       "  'cv_scores': array([0.5050246, 0.5037485]),\n",
       "  'avg_score': 0.5043865498028958,\n",
       "  'feature_names': ('3',)},\n",
       " 4: {'feature_idx': (0, 1),\n",
       "  'cv_scores': array([0.5022019 , 0.51371453]),\n",
       "  'avg_score': 0.5079582156770654,\n",
       "  'feature_names': ('0', '1')},\n",
       " 5: {'feature_idx': (0, 2),\n",
       "  'cv_scores': array([0.51318981, 0.51302453]),\n",
       "  'avg_score': 0.5131071659204627,\n",
       "  'feature_names': ('0', '2')},\n",
       " 6: {'feature_idx': (0, 3),\n",
       "  'cv_scores': array([0.50586411, 0.5099452 ]),\n",
       "  'avg_score': 0.5079046578999561,\n",
       "  'feature_names': ('0', '3')},\n",
       " 7: {'feature_idx': (1, 2),\n",
       "  'cv_scores': array([0.50846494, 0.51129598]),\n",
       "  'avg_score': 0.5098804586833956,\n",
       "  'feature_names': ('1', '2')},\n",
       " 8: {'feature_idx': (1, 3),\n",
       "  'cv_scores': array([0.50697546, 0.5050883 ]),\n",
       "  'avg_score': 0.5060318769443621,\n",
       "  'feature_names': ('1', '3')},\n",
       " 9: {'feature_idx': (2, 3),\n",
       "  'cv_scores': array([0.50668465, 0.50556135]),\n",
       "  'avg_score': 0.5061229990228089,\n",
       "  'feature_names': ('2', '3')},\n",
       " 10: {'feature_idx': (0, 1, 2),\n",
       "  'cv_scores': array([0.51139737, 0.51604878]),\n",
       "  'avg_score': 0.5137230781191187,\n",
       "  'feature_names': ('0', '1', '2')},\n",
       " 11: {'feature_idx': (0, 1, 3),\n",
       "  'cv_scores': array([0.50753753, 0.51232158]),\n",
       "  'avg_score': 0.5099295581616332,\n",
       "  'feature_names': ('0', '1', '3')},\n",
       " 12: {'feature_idx': (0, 2, 3),\n",
       "  'cv_scores': array([0.51615706, 0.51005798]),\n",
       "  'avg_score': 0.5131075195405539,\n",
       "  'feature_names': ('0', '2', '3')},\n",
       " 13: {'feature_idx': (1, 2, 3),\n",
       "  'cv_scores': array([0.51367245, 0.50796175]),\n",
       "  'avg_score': 0.510817099828599,\n",
       "  'feature_names': ('1', '2', '3')},\n",
       " 14: {'feature_idx': (0, 1, 2, 3),\n",
       "  'cv_scores': array([0.51472371, 0.50640877]),\n",
       "  'avg_score': 0.5105662399453947,\n",
       "  'feature_names': ('0', '1', '2', '3')}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs1.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efs1.best_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'v4'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat= X_train.columns[list(efs1.best_idx_)]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.5433561866210962\n",
      "Test set\n",
      "Random Forests roc-auc: 0.5253970921093112\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of classifier using selected features\n",
    "\n",
    "run_randomForests(X_train[selected_feat].fillna(0),\n",
    "                  X_test[selected_feat].fillna(0),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression도 동일하게 진행한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
