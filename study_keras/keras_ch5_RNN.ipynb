{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch5. RNN\n",
    "---------\n",
    "\n",
    "**예제 코드가 전혀 실행되지 않음**\n",
    "- 굳이 이 책으로 해야 할 필요가 있나 싶다.\n",
    "- 다른 책으로 해야 겠다\n",
    "\n",
    "recurrent nueral network(RNN)\n",
    "- 순환신경망\n",
    "- 계층의 출력이 순환하는 인공신경망\n",
    "- 순환 방식은 은닉 계층의 결과가 다음 계층으로 넘어갈 뿐 아니라 자기 계층으로 다시 들어오는 구조\n",
    "- 따라서 시계열 정보처럼 앞뒤 신호가 서로 상관도가 있는 경우 성능을 더 높일 수 있다.\n",
    "\n",
    "![alt text](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile23.uf.tistory.com%2Fimage%2F2578204757AC186F2A70DC)\n",
    "\n",
    "목차\n",
    "- RNN의 이해\n",
    "- 문장을 판별하는 RNN-LSTM\n",
    "- 시계열 신호열 예측에 RNN-LSTM활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 RNN의 원리\n",
    "\n",
    "#### 5.1.1 RNN의 개념과 구조\n",
    "\n",
    "RNN은 신호를 순환하여 시계열 신호와 같이 상호 관계가 있는 신호를 처리하는 구조\n",
    "- 단순한 방식으로 구현하면 경우에 따라서 학습이 제대로 이뤄지지 않을 수 있음\n",
    "- 출력된 신호가 계속 순환하면 활성화 함수를 반복적으로 거치게 되어서 gradient를 구하기 어려워짐\n",
    "  - 학습 방향을 결정하는 연산자인 gradient는 활성화 함수와 연계\n",
    "  - 신호가 커지면 포화가 되는 특성이 있기 때문에 약간만 입력값이 커져도 미분값이 매우 작아짐\n",
    "  - 이 때 순환하여 activation function이 반복해서 지나다 보면 미분값이 0에 매우 가까워져 학습이 어려워짐\n",
    "  - 따라서 이런 문제를 없애려면 곱셉보다는 덧셈을 사용해 과거의 값을 처리하는 것이 유리\n",
    "- 위의 기본적인 RNN의 문제점을 개선하고자 제안된 것이 LSTM임\n",
    "\n",
    "#### 5.1.2 LSTM 구조 및 동작\n",
    "\n",
    "<img src ='image/Keras_RNN_LSTM_structure_example.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 조절 벡터, 망각 벡터, 출력 조절 벡터를 이용해 입력과 출력 신호를 gating(게이팅) 함\n",
    "- 게이팅은 신호의 양을 조정해주는 기법\n",
    "- 입력 조절 벡터는 입력 신호가 tanh 활성화 함수의 완전 연결 계층을 거친 이후의 값을 조절\n",
    "- 망각 벡터는 과거 입력의 일부를 현재 입력에 반영\n",
    "- 그리고 출력 조절 벡터는 과거의 값과 수정된 입력값을 고려해 tanh 활성화 함수로 게이팅을 수행\n",
    "- 게이팅된 최종 결과가 출력 조절 벡터로 나가게 됨\n",
    "- 이 최종 결과는 데이터 처리를 위한 tanh 계층 그리고 게이팅을 위한 새 시그모이드 계층에 다시 입력으로 들어감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 문장을 판별하는 LSTM 구현\n",
    "\n",
    "LSTM을 이용하여 문장의 의미를 이해 해보는 시간\n",
    "\n",
    "1. 라이브러리 임포트\n",
    "2. 데이터 준비\n",
    "3. 모델링\n",
    "4. 학습 및 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.preprocessing import sequence\n",
    "- pad_sequence()와 같은 sequence를 처리하는 함수를 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 데이터 준비\n",
    "- IMDB를 사용\n",
    "- 영화평과 이진화된 영화 평점 정보(추천=1, 미추천=0)\n",
    "\n",
    "\n",
    "**코드가 실행이 안 된다** -.-\n",
    "- https://github.com/jskDr/keraspp/blob/master/ex5_1_lstm_imdb_cl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, max_features=20000, maxlen=80):\n",
    "#         # save np.load\n",
    "#         np_load_old = np.load\n",
    "\n",
    "#         # modify the default parameters of np.load\n",
    "#         np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "        \n",
    "        (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "        \n",
    "#         # restore np.load for future normal usage\n",
    "#         np.load = np_load_old\n",
    "        \n",
    "        x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "        x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "        \n",
    "        # 데이터 셋에 있는 문장들의 길이가 다르므로, LSTM이 처리하기 적합하도록 길이를 통일하는 작업 진행\n",
    "        # 문장의 최대 길이는 80\n",
    "        # 문장이 80보다 길이가 작으면 0으로 채움\n",
    "        # value라는 argu로 값을 채울 수 있음\n",
    "        \n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(models.Model):\n",
    "    def __init__(self, max_features, maxlen):\n",
    "#         model = Sequential()\n",
    "#         model.add(layers.Embedding(max_features, 128))\n",
    "#         model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#         model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # 입력에 각 샘플은 80개의 원소로 된 1차원 신호열이었지만 임베딩 계층을 통과하면서 각 단어가 128의 길이를 가지는 벡터로 바뀌면서 입력 데이터의 모양이 80 x 128로 변경\n",
    "        x = layers.Input((maxlen, ))\n",
    "        h = layers.Embedding(max_features, 128)(x)\n",
    "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
    "        y = layers.Dense(1, activation='sigmoid')(h)\n",
    "        super().__init__(x, y)\n",
    "\n",
    "        # try using different optimizers and different optimizer configs\n",
    "        # 긍정인지 부정인지에 대한 이진 판별값을 출력으로 다루므로 loss function은 'binary_crossentropy'\n",
    "        self.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self,\n",
    "                 max_features=20000,\n",
    "                 maxlen=80):\n",
    "        self.data = Data(max_features, maxlen)\n",
    "        self.model = RNN_LSTM(max_features, maxlen)\n",
    "\n",
    "    def run(self, epochs=3, batch_size=32):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        print('Training stage')\n",
    "        print('==============')\n",
    "        model.fit(data.x_train, data.y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(data.x_test, data.y_test))\n",
    "\n",
    "        score, acc = model.evaluate(data.x_test, data.y_test,\n",
    "                                    batch_size=batch_size)\n",
    "        print('Test performance: accuracy={0}, loss={1}'.format(acc, score))\n",
    "\n",
    "\n",
    "def main():\n",
    "    m = Machine()\n",
    "    m.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'international-airline-passengers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94452c450a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-94452c450a31>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmachine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-94452c450a31>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-94452c450a31>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, D)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'international-airline-passengers.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mdata_dn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-94452c450a31>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'international-airline-passengers.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipfooter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m   1131\u001b[0m                                      engine=engine))\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2223\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[1;32m   2224\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2225\u001b[0;31m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[1;32m   2226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'international-airline-passengers.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from keras import models, layers\n",
    "import seaborn as sns\n",
    "\n",
    "from kerasapp import skeras\n",
    "\n",
    "\n",
    "def main():        \n",
    "    machine = Machine() \n",
    "    machine.run(epochs=400)\n",
    "\n",
    "\n",
    "class Machine():\n",
    "    def __init__(self):\n",
    "        self.data = Dataset()\n",
    "        shape = self.data.X.shape[1:]\n",
    "        self.model = rnn_model(shape)\n",
    "        \n",
    "    def run(self, epochs=400):\n",
    "        d = self.data\n",
    "        X_train, X_test, y_train, y_test = d.X_train, d.X_test, d.y_train, d.y_test\n",
    "        X, y = d.X, d.y\n",
    "        m = self.model \n",
    "        h = m.fit(X_train, y_train, epochs=epochs, validation_data=[X_test, y_test], verbose=0)\n",
    "\n",
    "        skeras.plot_loss(h)\n",
    "        plt.title('History of training')\n",
    "        plt.show()\n",
    "\n",
    "        yp = m.predict(X_test)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))\n",
    "        plt.plot(yp, label='Origial')\n",
    "        plt.plot(y_test, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('Validation Results')\n",
    "        plt.show()\n",
    "\n",
    "        yp = m.predict(X_test).reshape(-1)\n",
    "        print('Loss:', m.evaluate(X_test, y_test))  \n",
    "        print(yp.shape, y_test.shape)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['Sample'] = list(range(len(y_test))) * 2\n",
    "        df['Normalized #Passengers'] = np.concatenate([y_test, yp], axis=0)\n",
    "        df['Type'] = ['Original'] * len(y_test) + ['Prediction'] * len(yp)\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.barplot(x=\"Sample\", y=\"Normalized #Passengers\", \n",
    "                    hue=\"Type\", data=df)\n",
    "        plt.ylabel('Normalized #Passengers')\n",
    "        plt.show()\n",
    "        \n",
    "        yp = m.predict(X)\n",
    "\n",
    "        plt.plot(yp, label='Origial')\n",
    "        plt.plot(y, label='Prediction')\n",
    "        plt.legend(loc=0)\n",
    "        plt.title('All Results')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def rnn_model(shape):\n",
    "    m_x = layers.Input(shape=shape) #X.shape[1:]\n",
    "    m_h = layers.LSTM(10)(m_x)\n",
    "    m_y = layers.Dense(1)(m_h)\n",
    "    m = models.Model(m_x, m_y)\n",
    "    \n",
    "    m.compile('adam', 'mean_squared_error')\n",
    "    \n",
    "    m.summary()\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, fname='international-airline-passengers.csv', D=12):\n",
    "        data_dn = load_data(fname=fname)\n",
    "        X, y = get_Xy(data_dn, D=D)\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "        \n",
    "        self.X, self.y = X, y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = X_train, X_test, y_train, y_test   \n",
    "\n",
    "\n",
    "def load_data(fname='international-airline-passengers.csv'):\n",
    "    dataset = pd.read_csv(fname, usecols=[1], engine='python', skipfooter=3)\n",
    "    data = dataset.values.reshape(-1)\n",
    "    plt.plot(data)\n",
    "    plt.xlabel('Time'); plt.ylabel('#Passengers')\n",
    "    plt.title('Original Data')\n",
    "    plt.show()\n",
    "\n",
    "    # data normalize\n",
    "    data_dn = (data - np.mean(data)) / np.std(data) / 5\n",
    "    plt.plot(data_dn)\n",
    "    plt.xlabel('Time'); plt.ylabel('Normalized #Passengers')\n",
    "    plt.title('Normalized data by $E[]$ and $5\\sigma$')\n",
    "    plt.show()\n",
    "    \n",
    "    return data_dn\n",
    "\n",
    "\n",
    "def get_Xy(data, D=12):\n",
    "    # make X and y\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    N = len(data)\n",
    "    assert N > D, \"N should be larger than D, where N is len(data)\"\n",
    "    for ii in range(N-D-1):\n",
    "        X_l.append(data[ii:ii+D])\n",
    "        y_l.append(data[ii+D])\n",
    "    X = np.array(X_l)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.array(y_l)\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
